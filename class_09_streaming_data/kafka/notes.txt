STREAMING
- Streaming is the process of publishing data from some source, like web logs, sensor data or something like that and actually getting that in a scalable manner published into your cluster where you can actually do some processing on it.
- How new data get into your cluster? Especially if it is a "big data"?
  - New log entries from your web servers
  - New sensor data from your IoT system
  - New stock trades
- Streaming lets you publish this data, in real time, to your cluster
  - And you can even process it in real time as it comes in.
- Two problems
  - How to get data from many different sources flowing into your cluster
  - Processing when it is there

KAFKA
- Is a general purpose publish/subscribe messaging system.
- Kafka servers store all incoming messages from publishers for some period of time, and publishes them to a stream of data called Topic.
- Kafka consumers subscribe to one or more topics, and receive data as it is published
- A stream / topic can have many different consumers, all with their own position in the stream maintened
- It is not just for Hadoop
- Kafka may be distributed among many processes on many servers
  - Will distribute the storage of stream data as well
- Consumers may also be distributed
  - Consumers of the same group will have messages distributed amongst them
  - Consumers of different groups will get their own copy of each messages
  
