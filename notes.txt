Ambari - GUI for hadoop managament based on its RESTful APIs
Hive - facilitates reading, writing and managing large datasets residing in distributed storage using SQL.

hadoop is an open source software platfom for distributed storage and distributed processing of very large data seets on computer cluster built from commodity hardware.

MapReduce is for distributed processing.
Hadoop was the name of Doug Cutting kid's toy elephant.

Horizontal scaling: add more computers onto your cluster. Horizontal scaling is linear.
Vertical scaling: add more power through adding more resources to your computer (CPU, RAM)


Why Hadoop?
 - data is too damn big - terabytes per day.
 - vertical scaling doesn't cut it.
 - Hadoop isn't only for batch processing anymore.

*** Hadoop stack ***
* HDFS: Hadoop Distributed File System. (Evolution of GFS – Google File System). Allow us to distribute the storage of big data across our cluster of computers. It make all the hard drivers on all our machines look like an giant file system.
It also maintain copies of data between nodes, só if a computer dies, no data is lost.
* YARN: Yet Another Resource Negotiator. It manages resources on your computing cluster.
* MapReduce: Programming metaphor or programming model that allows you to process your data across an entire cluster. It consists of mappers and reducers. Mappers have the ability to transform your data in parallel across your entire computing cluster in a very efficient manner. Reducers are what aggregate that data together.
* Pig: Pig is a very high level programming scripting language that sits on top of MapReduce. It allows you to write simple scripts that look like SQL or to chain together queries and get complex answers without writing any python or java code.
* Hive: It has some similarity with Pig. It sits on top of MapReduce and look like a SQL Database. Hive is a way to write SQL queries and making this distributed.
* Ambari: Gives you a view of your cluster and let you visualize what’s running on your cluster, which systems are using how much resources, views that allow you to do things like execute hive/pig queries or import databases.
* Mesos: Is an alternative for yarn. 
* Spark: One of the most exciting technologies in the Hadoop ecosystem. It is sitting on the same level of MapReduce. And it sits on top of Yarn or Mesos. It can run queries on your data and like MapReduce it requires some programming, and you need to write your Spark queries using Python, Java or Scala (Scala is preferred). Spark is extremely fast and it is under a lot of development. If you need to quickly, efficiently and reliably process data on your cluster, Spark is a really good choice. It is versatile. It can handle SQL queries. It can do machine learning across an entire cluster of information. It can handle streaming data in real time and other cool stuff.
* Tez: It is similar to Spark and use similar techniques as Spark such as “acyclic graph” and it gives Tez a leg up on what MapReduce does because it can produce more optimal plans for actually execute queries. Tez is commonly used with hive.
* HBase: It a way of exposing data on your cluster to a transactional platforms. HBase is what we call NoSQL database and it is a columnar data store. It is a fast database meant for very large transaction rates.
* Apache Storm: It is used for processing streaming data in real time (Spark streaming solves the same problem). So, with apache storm processing does not need to be batch. It can update your machine learning models or transform data into a database in real time.
* Oozie: It is a scheduler. You can schedule jobs onto your cluster.
* Zookeeper: It is a technology for coordinating everything on your cluster. It is used for keeping track of which nodes are up and which nodes down. It can keep track of shared states across your cluster that different applications can use. Many applications rely on Zookeeper to keep data consistent and reliable even when a cluster goes down. It can keep track of what cluster is the master node, for example.

*** Data ingestion ***
* Sqoop: It is a way of tying your Hadoop database into a relational database (anything that can talk to ODBC or JDBC can be transformed by sqoop to HDFS). Sqoop is basically a connector between Hadoop and your legacy databases.
* Flume: It a way of transporting web logs at a very large scale and very reliably onto your cluster. If you have a fleet of web servers Flume can listen to the web logs coming in from those web servers in real time and publish then into your cluster in real time for processing by storm or spark streaming.
* Apache Kafka: Similar to Flume, but it for more general purpose. It can basically collect data from any sort from a cluster of PCs, from a cluster of web servers or whatever it is and broadcast that data into your Hadoop cluster as well.

*** External data storage ***
* HBase could fit here
* MySQL (or any SQL Database): You can import or export data from Hadoop.
* Cassandra / MongoDB: Columnar data stores that are good choices for exposing your data for real time usage.

*** Query Engines ***
* Hive could fit here.
* Apache Drill: Allows you to write SQL Queries that will work across a wide range of NoSQL databases. It can talk to your MongoDB, Cassandra and maybe HBase and tie those results all together.
* Hue: A way of interactively creating queries that works well with Hive and HBase. For Cloudera it takes the role of Ambari as the thing that sits on top of everything and lets you visualize and execute queries on the Hadoop cluster as well. 
* Apache Phoenix: Similar to Drill it allows you to write SQL queries across the entire range of data stores technologies you might have. It goes one step further and it gives you ACID (Atomicity, Consistency, Isolation and Durability) and OLTP (Online Transaction Processing).
* Presto: Another way to execute queries across your entire cluster.
* Apache Zeppelin: Notebook approach to the User Interface and how you interact with your Hadoop cluster.

_____
*** HDFS
- optimized for handling big files. It breaks those files into blocks (128 megabytes by default). You can store files that are bigger then a individual harddrive can store. In this manner hadoop allows the distributed processing of these large files.
- it has a single name node and this name node is what keep track of where all those blocks alive. It has an “edit log” that maintains a record of what is being created, what is being modified and what is being stored.
- there are some manners to avoid a single point of failure on a name node (which must be unique – except for name node federation)
  - back up metadata: name nodes writes to local disks and NFS.
  - secondary namenode: maintains merged copy of edit log you can restore from
  - hdfs federation: each namenode manages a specific namespace volume
  - hdfs high availability: hot standby namenode using shared edit log, zoodeeper tracks active namenode, uses extreme measures to ensure only one namenode is used at a time (like cut power off).
- How to use HDFS?
  - UI (Ambari)
  - Command Line Interface 
  - HTTP / HDFS Proxies
  - Java interface
  - NFS Gateway

*** MapReduce
- MapReduce is one of the core pieces of hadoop.
- Distributes the processing of your data on your cluster.
- Divides you data up into partitions that are Mapped (transformed) and Reduced (Aggregated) by mapper and reducer funcions you define.
- Resilient to failure - an application master monitor your mappers and reducers on each partition. 
- Reduce -> Aggregate data

*** Pig
- Pig is a scripting language called Pig Latin built on top of Hadoop and MapReduce that lets you create MapReduce jobs without having to write Maps and Reducers.
- Pig Latin lets you use SQL-like syntax to define your map and reduce steps.
- Highly extensible with user-defined functions (UDF's).
- Pig sits on top of MapReduce or Tez. MapReduce or Tez sits on top of Yarn and Yarn sits on top of HDFS.
- You can run Pig by using
  - Grunt (command line interpreter)
  - Script file
  - Ambari / Hue