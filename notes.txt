Ambari - GUI for hadoop managament based on its RESTful APIs
Hive - facilitates reading, writing and managing large datasets residing in distributed storage using SQL.

hadoop is an open source software platfom for distributed storage and distributed processing of very large data seets on computer cluster built from commodity hardware.

MapReduce is for distributed processing.
Hadoop was the name of Doug Cutting kid's toy elephant.

Horizontal scaling: add more computers onto your cluster. Horizontal scaling is linear.
Vertical scaling: add more power through adding more resources to your computer (CPU, RAM)


Why Hadoop?
 - data is too damn big - terabytes per day.
 - vertical scaling doesn't cut it.
 - Hadoop isn't only for batch processing anymore.

*** Hadoop stack ***
* HDFS: Hadoop Distributed File System. (Evolution of GFS – Google File System). Allow us to distribute the storage of big data across our cluster of computers. It make all the hard drivers on all our machines look like an giant file system.
It also maintain copies of data between nodes, só if a computer dies, no data is lost.
* YARN: Yet Another Resource Negotiator. It manages resources on your computing cluster.
* MapReduce: Programming metaphor or programming model that allows you to process your data across an entire cluster. It consists of mappers and reducers. Mappers have the ability to transform your data in parallel across your entire computing cluster in a very efficient manner. Reducers are what aggregate that data together.
* Pig: Pig is a very high level programming scripting language that sits on top of MapReduce. It allows you to write simple scripts that look like SQL or to chain together queries and get complex answers without writing any python or java code.
* Hive: It has some similarity with Pig. It sits on top of MapReduce and look like a SQL Database. Hive is a way to write SQL queries and making this distributed.
* Ambari: Gives you a view of your cluster and let you visualize what’s running on your cluster, which systems are using how much resources, views that allow you to do things like execute hive/pig queries or import databases.
* Mesos: Is an alternative for yarn. 
* Spark: One of the most exciting technologies in the Hadoop ecosystem. It is sitting on the same level of MapReduce. And it sits on top of Yarn or Mesos. It can run queries on your data and like MapReduce it requires some programming, and you need to write your Spark queries using Python, Java or Scala (Scala is preferred). Spark is extremely fast and it is under a lot of development. If you need to quickly, efficiently and reliably process data on your cluster, Spark is a really good choice. It is versatile. It can handle SQL queries. It can do machine learning across an entire cluster of information. It can handle streaming data in real time and other cool stuff.
* Tez: It is similar to Spark and use similar techniques as Spark such as “acyclic graph” and it gives Tez a leg up on what MapReduce does because it can produce more optimal plans for actually execute queries. Tez is commonly used with hive.
* HBase: It a way of exposing data on your cluster to a transactional platforms. HBase is what we call NoSQL database and it is a columnar data store. It is a fast database meant for very large transaction rates.
* Apache Storm: It is used for processing streaming data in real time (Spark streaming solves the same problem). So, with apache storm processing does not need to be batch. It can update your machine learning models or transform data into a database in real time.
* Oozie: It is a scheduler. You can schedule jobs onto your cluster.
* Zookeeper: It is a technology for coordinating everything on your cluster. It is used for keeping track of which nodes are up and which nodes down. It can keep track of shared states across your cluster that different applications can use. Many applications rely on Zookeeper to keep data consistent and reliable even when a cluster goes down. It can keep track of what cluster is the master node, for example.

*** Data ingestion ***
* Sqoop: It is a way of tying your Hadoop database into a relational database (anything that can talk to ODBC or JDBC can be transformed by sqoop to HDFS). Sqoop is basically a connector between Hadoop and your legacy databases.
* Flume: It a way of transporting web logs at a very large scale and very reliably onto your cluster. If you have a fleet of web servers Flume can listen to the web logs coming in from those web servers in real time and publish then into your cluster in real time for processing by storm or spark streaming.
* Apache Kafka: Similar to Flume, but it for more general purpose. It can basically collect data from any sort from a cluster of PCs, from a cluster of web servers or whatever it is and broadcast that data into your Hadoop cluster as well.

*** External data storage ***
* HBase could fit here
* MySQL (or any SQL Database): You can import or export data from Hadoop.
* Cassandra / MongoDB: Columnar data stores that are good choices for exposing your data for real time usage.

*** Query Engines ***
* Hive could fit here.
* Apache Drill: Allows you to write SQL Queries that will work across a wide range of NoSQL databases. It can talk to your MongoDB, Cassandra and maybe HBase and tie those results all together.
* Hue: A way of interactively creating queries that works well with Hive and HBase. For Cloudera it takes the role of Ambari as the thing that sits on top of everything and lets you visualize and execute queries on the Hadoop cluster as well. 
* Apache Phoenix: Similar to Drill it allows you to write SQL queries across the entire range of data stores technologies you might have. It goes one step further and it gives you ACID (Atomicity, Consistency, Isolation and Durability) and OLTP (Online Transaction Processing).
* Presto: Another way to execute queries across your entire cluster.
* Apache Zeppelin: Notebook approach to the User Interface and how you interact with your Hadoop cluster.

_____
*** HDFS
- optimized for handling big files. It breaks those files into blocks (128 megabytes by default). You can store files that are bigger then a individual harddrive can store. In this manner hadoop allows the distributed processing of these large files.
- it has a single name node and this name node is what keep track of where all those blocks alive. It has an “edit log” that maintains a record of what is being created, what is being modified and what is being stored.
- there are some manners to avoid a single point of failure on a name node (which must be unique – except for name node federation)
  - back up metadata: name nodes writes to local disks and NFS.
  - secondary namenode: maintains merged copy of edit log you can restore from
  - hdfs federation: each namenode manages a specific namespace volume
  - hdfs high availability: hot standby namenode using shared edit log, zoodeeper tracks active namenode, uses extreme measures to ensure only one namenode is used at a time (like cut power off).
- How to use HDFS?
  - UI (Ambari)
  - Command Line Interface 
  - HTTP / HDFS Proxies
  - Java interface
  - NFS Gateway

*** MapReduce
- MapReduce is one of the core pieces of hadoop.
- Distributes the processing of your data on your cluster.
- Divides you data up into partitions that are Mapped (transformed) and Reduced (Aggregated) by mapper and reducer funcions you define.
- Resilient to failure - an application master monitor your mappers and reducers on each partition. 
- Reduce -> Aggregate data

*** Pig
- Pig is a scripting language called Pig Latin built on top of Hadoop and MapReduce that lets you create MapReduce jobs without having to write Maps and Reducers.
- Pig Latin lets you use SQL-like syntax to define your map and reduce steps.
- Highly extensible with user-defined functions (UDF's).
- Pig sits on top of MapReduce or Tez. MapReduce or Tez sits on top of Yarn and Yarn sits on top of HDFS.
- You can run Pig by using
  - Grunt (command line interpreter)
  - Script file
  - Ambari / Hue
- Pig Latin Commands:
  - LOAD: Loads a dataset from a file in HDFS.
  - STORE: Saves a dataset to a HDFS file.
  - DUMP: Prints a dataset to output.
  - FILTER: Filter a dataset based on a boolean expression.
  - DISTINCT: Gives you back the unique values within a relation.
  - FOREACH/GENERATE: Creates a new relation from an existing one through one line at a time and transforming it someway.
  - MAPREDUCE: Lets you call explicit mappers and reducers on a relation.
  - STREAM: Used for extensibility. You can stream results of Pig out to a process and just use StdIn and StdOut.
  - SAMPLE: Can be used to create a random sample from your relation.
  - JOIN: Join different datasets into one.
  - COGROUP: Creates a separate tuple for each key and creates a nested structure.
  - GROUP: A way of grouping together all the different values associated with the given key that you specified (It is doing a reduce). 
  - CROSS: Lets you do all the combinations between two relations (Cartesian product).
  - CUBE: Similar to CROSS, but bigger.
  - ORDER: Sort your dataset.
  - RANK: Similar to sort, but does not change the order. It actually gives a Rank for each row.
  - LIMIT: Handy if you as just experimenting or debbugging. You can limit whatever dataset or relationship.
  - UNION: Takes two relations and squishes them together.
  - SPLIT: Takes a relation and splits it up into more than one relation.
- Pig latin Diagnostics:
  - DESCRIBE: Describe what pigs think is inside of a given relation - names and types.
  - EXPLAIN: A simple explain plan so it can actually give a little insight into how pig intends to execute a given query.
  - ILLUSTRATE: Goes a little further than explain. It takes a sample from each relation and shows you exactly what it is doint with each piece of data.
- Pig Latin UDF - User Defined Function:
  - REGISTER: Register a .jar file that contains a UDF.
  - DEFINE: Assign names to those functions imported by a .jar file.
  - IMPORT: Used for importing macros for pig file so you can actually have reusable bits of Pig code.
- Pig Latin other functions and loaders.
  - AVG: Average of a bunch of stuff in a bag.
  - CONCAT, COUNT, MAX, MIN, SIZE, SUM: Self explanatory.
  - PigStorage: User for dellimited data.
  - TextLoader: Just loads up one line of input data or output data per row.
  - JsonLoader: Used for load JSON files.
  - AvroStorage: Saves a dataset into .AVRO file.
  - ParquetLoader: Used for load PARQUET files.
  - OrcStorage: Saves a dataset into .ORC file.
  - HBaseStorage: Integrates Pig with HBase.